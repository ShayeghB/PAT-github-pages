# -*- coding: utf-8 -*-
"""PAT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CLc1sSEKUdOEDvd9s7XBe4uttso8-YG3

# Requirements
"""

HOME = '/content/drive/MyDrive/PAT_code_to_share'

import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
from google.colab import drive
drive.mount('/content/drive')

!pip install hazm
!pip install num2fawords

!cp -r "$HOME/Library/" .
!cp -r "$HOME/Data/urban_hierarchy.json" .
!unzip "$HOME/Data/RawDataZips/mrud.zip"
!unzip "$HOME/Data/RawDataZips/dehKhoda.zip"
!unzip "$HOME/Data/RawDataZips/sina_post_compact_addresses.zip"
!unzip "$HOME/Data/RawDataZips/sina_post_address_details.zip"

import re
import json
import numpy as np
import pandas as pd
from Library.Utils import *
from Library.PAT import PAT
from tqdm.notebook import tqdm
from Library.NBTree import NB3
from Library.NaiveBayes import NBLM,PNBLM
from Library.Preprocessor import Preprocessor
from Library.ApartementDetailExtractor import ADE
from Library.AccurateRestorer import AccurateRestorer

"""# Preprocessor

## Train
"""

dehKhoda = pd.read_csv('dehKhoda.csv', header=None)
dehKhoda = pd.DataFrame({'word': np.unique(dehKhoda[0])})
dehKhoda = dehKhoda[~dehKhoda['word'].apply(lambda x: '…' in x)]
dehKhoda = dehKhoda[dehKhoda['word'].apply(len) > 2]
dehKhoda = dehKhoda[dehKhoda['word'].apply(lambda x: x.count(' ')) == 0]
dehKhoda = dehKhoda[dehKhoda['word'].apply(lambda x: '-' not in x)]
dehKhoda_short_words = pd.read_csv('./Library/ValidShortWords.csv')
suffix = dehKhoda_short_words[dehKhoda_short_words.suffix].word.values.tolist()
prefix = dehKhoda_short_words[dehKhoda_short_words.prefix].word.values.tolist()
dehKhoda_short_words = dehKhoda_short_words[['word']]
dehKhoda = dehKhoda.append(dehKhoda_short_words, ignore_index=True)
dehKhoda = dehKhoda.drop_duplicates(subset=['word'])

mrud = pd.read_csv('mrud.csv',usecols=['address','parcel'])
mrud = mrud[~mrud.parcel.isna()]
mrud = mrud[~mrud.address.isna()]
mrud = mrud[~mrud.address.apply(lambda x: 'کژی' in x or '«یٍ«' in x)]
sina_post = pd.read_csv('sina_post_compact_addresses.csv')
sina_post = sina_post[~sina_post.address.str.isdigit()]
sina_post.address = sina_post.address.apply(lambda x: re.sub('/[0-9]+،', '،', x))
mrud_and_sina_post = pd.concat([mrud, sina_post])
mrud_and_sina_post_text = ' split '.join(pd.unique(mrud_and_sina_post.address))

P = Preprocessor(dehKhoda.word, prefix, suffix)
P.train(mrud_and_sina_post_text, just_look_words=True)
processed_addresses = P.batch_run(mrud_and_sina_post.address.values)
processed_addresses = pd.DataFrame(processed_addresses.items(), columns=['address', 'clean'])
processed_addresses = mrud_and_sina_post.merge(processed_addresses, on='address', how='left')

"""### Save"""

processed_addresses.to_csv(f'{HOME}/Data/processed_addresses.csv', index=False)
P.save(f'{HOME}/Models/prep')

"""## Load"""

P = Preprocessor.load(f'{HOME}/Models/prep')
processed_addresses = pd.read_csv(f'{HOME}/Data/processed_addresses.csv')

"""# NB3"""

class ParishLayerLM(NBLM) :
  def __init__(self) :
    super().__init__(smooth_factor=1/100000)

class AvenueLayerLM(PNBLM) :
  def __init__(self) :
    super().__init__(smooth_factor=1/100000, idf_power=1.5)

"""## Train"""

layers = ['parish','preaven_type','preaven_name','avenue_type','avenue_name']
using_columns = layers+['parcel']
post_details = pd.read_csv('sina_post_address_details.csv',usecols=using_columns)
for c in layers :
  post_details[c] = post_details[c].apply(lambda x: x.replace('/','_') if type(x)==type('') else str(x))
  post_details[c][post_details[c] == 'nan'] = ''
post_details['avenue'] = post_details.preaven_type.str.\
                     cat(post_details.preaven_name, sep=' ').str.\
                     cat(post_details.avenue_type, sep=' ').str.\
                     cat(post_details.avenue_name, sep=' ')
post_details = post_details.drop(columns=layers[1:])

df = processed_addresses[~processed_addresses.address.str.isdigit()]
df = df.merge(post_details, how='left', on='parcel')

nb3 = NB3("تهران",
          layers             = ['parish','avenue','clean'],
          data               = df,
          actual_labels_freq = post_details,
          NB_class           = [ParishLayerLM, AvenueLayerLM])

"""### Save"""

nb3.save(f'{HOME}/Models/nb3/')

"""## Load"""

nb3 = NB3.load(f'{HOME}/Models/nb3/',
                  parts_count=8, NB_class=[ParishLayerLM, AvenueLayerLM])

"""# BDE & AR"""

ade = ADE()
urban_hierarchy = json.load(open('urban_hierarchy.json'))
ar = AccurateRestorer(layers=['parish','avenue'],
                      urban_hierarchy=urban_hierarchy,
                      preprocessor=P,
                      prob_keyword='probability',
                      label_cond_thresholds=np.exp(-150),
                      plateno_cond_thresholds=np.exp(-200)
                      )

"""# PAT"""

pat = PAT(P, nb3, ade, ar, urban_hierarchy)

pat['شریعتی - ظفر - اطلسی - سیفیه شرقی - پ ۲۸']

pat['بزرگراه ستاری - بلوار فردوس شرقی - خیابان ابراهیمی جنوبی - نبش کوچه ۱۲ - پلاک ۱۴ - واحد ۲۴']